{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting V1 Notes: 100%|█████████▉| 2821/2824 [00:05<00:00, 509.13it/s]\n"
     ]
    }
   ],
   "source": [
    "import openreview\n",
    "# client = openreview.api.OpenReviewClient(baseurl='https://api2.openreview.net')\n",
    "client = openreview.Client(baseurl='https://api.openreview.net')\n",
    "\n",
    "conference_id = 'NeurIPS.cc/2022/Conference'\n",
    "papers = client.get_all_notes(\n",
    "    invitation=f'{conference_id}/-/Blind_Submission',\n",
    "    details='directReplies', \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'zzDrPqn57DL',\n",
       " 'original': 'HXF7e04ViJI',\n",
       " 'number': 2310,\n",
       " 'cdate': 1652737363731,\n",
       " 'pdate': 1667239200000,\n",
       " 'odate': None,\n",
       " 'mdate': None,\n",
       " 'tcdate': 1652737363731,\n",
       " 'tmdate': 1720019111771,\n",
       " 'ddate': None,\n",
       " 'content': {'title': 'BEVFusion: A Simple and Robust LiDAR-Camera Fusion Framework',\n",
       "  'authorids': ['~Tingting_Liang2',\n",
       "   '~Hongwei_Xie1',\n",
       "   '~Kaicheng_Yu1',\n",
       "   '~Zhongyu_Xia1',\n",
       "   '~Zhiwei_Lin1',\n",
       "   '~Yongtao_Wang1',\n",
       "   '~Tao_Tang4',\n",
       "   '~Bing_Wang14',\n",
       "   '~Zhi_Tang2'],\n",
       "  'authors': ['Tingting Liang',\n",
       "   'Hongwei Xie',\n",
       "   'Kaicheng Yu',\n",
       "   'Zhongyu Xia',\n",
       "   'Zhiwei Lin',\n",
       "   'Yongtao Wang',\n",
       "   'Tao Tang',\n",
       "   'Bing Wang',\n",
       "   'Zhi Tang'],\n",
       "  'keywords': ['3D Object Detection',\n",
       "   \"bird's eye view perception\",\n",
       "   'camera-lidar fusion'],\n",
       "  'TL;DR': 'We introduce a simple LiDAR-camera fusion framework that overcomes the reliance of previous fusion methods on LiDAR through BEV space fusion.',\n",
       "  'abstract': 'Fusing the camera and LiDAR information has become a de-facto standard for 3D object detection tasks. Current methods rely on point clouds from the LiDAR sensor as queries to leverage the feature from the image space. However, people discovered that this underlying assumption makes the current fusion framework infeasible to produce any prediction when there is a LiDAR malfunction, regardless of minor or major. This fundamentally limits the deployment capability to realistic autonomous driving scenarios. In contrast, we propose a surprisingly simple yet novel fusion framework, dubbed BEVFusion, whose camera stream does not depend on the input of LiDAR data, thus addressing the downside of previous methods. We empirically show that our framework surpasses the state-of-the-art methods under the normal training settings. Under the robustness training settings that simulate various LiDAR malfunctions, our framework significantly surpasses the state-of-the-art methods by 15.7% to 28.9% mAP. To the best of our knowledge, we are the first to handle realistic LiDAR malfunction and can be deployed to realistic scenarios without any post-processing procedure. ',\n",
       "  'pdf': '/pdf/5d63ec7319335a25c1248983feeb89da3c8a3817.pdf',\n",
       "  'paperhash': 'liang|bevfusion_a_simple_and_robust_lidarcamera_fusion_framework',\n",
       "  'supplementary_material': '/attachment/4ee7223e46485fe1a8ab7735deba085b0d996156.pdf',\n",
       "  'venue': 'NeurIPS 2022 Accept',\n",
       "  'venueid': 'NeurIPS.cc/2022/Conference',\n",
       "  '_bibtex': '@inproceedings{\\nliang2022bevfusion,\\ntitle={{BEVF}usion: A Simple and Robust Li{DAR}-Camera Fusion Framework},\\nauthor={Tingting Liang and Hongwei Xie and Kaicheng Yu and Zhongyu Xia and Zhiwei Lin and Yongtao Wang and Tao Tang and Bing Wang and Zhi Tang},\\nbooktitle={Advances in Neural Information Processing Systems},\\neditor={Alice H. Oh and Alekh Agarwal and Danielle Belgrave and Kyunghyun Cho},\\nyear={2022},\\nurl={https://openreview.net/forum?id=zzDrPqn57DL}\\n}',\n",
       "  'community_implementations': '[![CatalyzeX](/images/catalyzex_icon.svg) 2 code implementations](https://www.catalyzex.com/paper/bevfusion-a-simple-and-robust-lidar-camera/code)'},\n",
       " 'forum': 'zzDrPqn57DL',\n",
       " 'referent': None,\n",
       " 'invitation': 'NeurIPS.cc/2022/Conference/-/Blind_Submission',\n",
       " 'replyto': None,\n",
       " 'readers': ['everyone'],\n",
       " 'nonreaders': [],\n",
       " 'signatures': ['NeurIPS.cc/2022/Conference'],\n",
       " 'writers': ['NeurIPS.cc/2022/Conference'],\n",
       " 'details': {'directReplies': [{'id': 'VrENUT1h6-',\n",
       "    'original': None,\n",
       "    'number': 1,\n",
       "    'cdate': 1656944073004,\n",
       "    'mdate': None,\n",
       "    'ddate': None,\n",
       "    'tcdate': 1656944073004,\n",
       "    'tmdate': 1656944073004,\n",
       "    'tddate': None,\n",
       "    'forum': 'zzDrPqn57DL',\n",
       "    'replyto': 'zzDrPqn57DL',\n",
       "    'invitation': 'NeurIPS.cc/2022/Conference/Paper2310/-/Official_Review',\n",
       "    'content': {'rating': '7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.',\n",
       "     'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.',\n",
       "     'summary': 'Towards the problem of current methods tend to fail at situations where hardware malfunctions, this paper presents a simple yet effective LiDAR-Camera fusion framework, namely BEVFusion. By disentangling camera pipeline from LiDAR network and using a dynamic fusion module, BEVFusion achieves SOTA performance and shows robustness against LiDAR or camera malfunction at the same time. An effective modification on the camera pipeline is also proposed to boost the final performance.',\n",
       "     'strengths_and_weaknesses': '## Strength\\n1. The paper is well written and easy to read.\\n2. Robustness of autonomous driving algorithms should be paid more attention to. This paper raises the issue and makes the attempt to addressing it.\\n3. Thorough experiments are performed. Claims are well-supported. SOTA performance is achieved on both normal and robust settings of nuScenes.\\n4. The clean design of the framework makes it easy to use any camera or LiDAR framework.\\n\\n\\n## Weaknesses\\n1. It is nice to see a simple yet effective module (dynamic fusion module) being proposed. But it would be nicer to provide some insights and analysis into the design itself. For example, by analyzing how would the fusion module work when facing incomplete LiDAR or camera inputs, we might gain some insights into the module design of CSF and AFS.\\n2. The experiments section does not provide runtime analysis, like inference time and memory footprint, and its comparison with other methods.',\n",
       "     'questions': '1. How does the baseline method in Table 7 fuse the features?\\n2. This does not affect my rating to the paper. Just out of curious, would BEVFusion still works when facing both camera and LiDAR malfunction?',\n",
       "     'limitations': 'The potential negative social impact is well discussed. But the limitation should be discussed more. For example, would the late-fusion style misses the opportunity to fuse intermediate LiDAR and camera features, and thus makes the pipeline suffer potential performance drop?',\n",
       "     'ethics_flag': 'No',\n",
       "     'ethics_review_area': [],\n",
       "     'soundness': '3 good',\n",
       "     'presentation': '3 good',\n",
       "     'contribution': '3 good',\n",
       "     'code_of_conduct': 'Yes'},\n",
       "    'signatures': ['NeurIPS.cc/2022/Conference/Paper2310/Reviewer_CaaL'],\n",
       "    'readers': ['everyone'],\n",
       "    'nonreaders': [],\n",
       "    'writers': ['NeurIPS.cc/2022/Conference',\n",
       "     'NeurIPS.cc/2022/Conference/Paper2310/Reviewer_CaaL']},\n",
       "   {'id': 'B8g021IaaAm',\n",
       "    'original': None,\n",
       "    'number': 2,\n",
       "    'cdate': 1657487808768,\n",
       "    'mdate': None,\n",
       "    'ddate': None,\n",
       "    'tcdate': 1657487808768,\n",
       "    'tmdate': 1660833391821,\n",
       "    'tddate': None,\n",
       "    'forum': 'zzDrPqn57DL',\n",
       "    'replyto': 'zzDrPqn57DL',\n",
       "    'invitation': 'NeurIPS.cc/2022/Conference/Paper2310/-/Official_Review',\n",
       "    'content': {'rating': '5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.',\n",
       "     'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.',\n",
       "     'summary': 'Most existing camera-lidar fusion work decorates lidar points with image features and then performs detection in 3D/BEV space. This work leverages recent Lift-Splat-Shoot work for cameras, which allows one to map both camera and lidar inputs to BEV space, before fusing and applying the detection head. ',\n",
       "     'strengths_and_weaknesses': 'Strengths: \\n- The proposed idea and its realization makes sense, and I am not aware of such published work (even though there seems to be concurrent similar work, since this seems a logical next step given the existence of LSS [52]). \\n- Details in the model seem well thought out. This include the extensions to LSS (Dual-Swin-Tiny architecture, ADP), as well as the layers in the dynamic fusion module. \\n- The experimental results show that this work is close to SOTA on nuScenes and that it affords significant model robustness in the case of lidar information missing compared to existing methods. \\n- The model details are pretty clearly explained. \\n\\nWeaknesses: \\n- Related work section is confusing in a few places and can be streamlined further. Examples: \\n1) The Camera detectors section contains a discussion of PointPillars, which is a purely Lidar method. \\n2) Range images are not really Euclidean space (see line 88) \\n3) 89: \"Recently, people start to exploit these two feature modalities to increase the representation power\" --> There is earlier work to do this, if I understand correctly the statement. E.g. [5] from the paper, or End-to-End Multi-View Fusion for 3D Object Detection in LiDAR Point Clouds, by Yin Zhou et al, CoRL 2019. \\n4) 90: \" Another line of work is to exploit the benefit of the bird’s eye view plane similar to the camera perception\" --> a lot of this work came before camera started exploiting the BEV view. \\n\\n- Intuitive explanations are lacking in a couple of instances: \\n1) Work does not explain the intuition why the model needs to be trained in two stages. What happens if it\\'s trained in a single stage? \\n2) 14: \"Note that we do not conduct data augmentation when multi-view image input is involved, while data augmentation plays a critical part in other cutting edge methods.\" Is this a limitation of camera fusion methods in general or something specifically lacking in your case? Can you please clarify? \\n\\n- It is unclear whether the approach is SOTA on nuScenes or not. Can you please explicitly contrast your performance relative to the nuScenes leaderboard (at least for the published approaches). When exploring that leaderboard myself, I see mentions of a method called BEVFusion that is SOTA but seems to be a different method? Assuming that method is different and already on the leaderboard, your naming may be confusing / too generic. \\n\\n- nuScenes is a dataset with particularly poor lidar (compared to other public datasets, such as Waymo Open Dataset, Argoverse2.0 etc). Results on at least one more dataset with high quality and longer-range lidar are highly desirable. The core issue of missing lidar points may be a lot less pertinent for more modern lidars. Also, as range increases beyond ~40m to 70-200m, the approach here may actually underperform lidar-painting approaches, since BEV view can start containing errors > 10m in the camera case making fusion in BEV space difficult. To this effect, analysis of the method performance as a function of object distance, relative to SOTA fusion methods for long distances will help. \\n\\n\\nLanguage: \\nThere are minor language issues and typos in the paper, it would benefit from another proofreading pass. \\n',\n",
       "     'questions': '- Is the approach SOTA on nuscenes or not?  Can you please explicitly contrast your performance relative to the nuScenes leaderboard (at least for the published approaches). \\n\\n- Can you provide an analysis of performance as a function of distance to object, and compare to a standard lidar approach and TransFusion/DeepFusion etc? \\n\\n- Can you please provide results on at least one more dataset with high quality Lidar such as the Waymo Open Dataset? \\n\\n- What is the latency of the approach and how does it compare to the baselines? ',\n",
       "     'limitations': 'See comment on weaknesses. Some core potential limitations of the existing method have not been fully explored. \\n\\nMy current rating is predicated on the assumption that a similar idea has not been published yet (not completely certain) and that I will receive reasonable responses to my questions. ',\n",
       "     'ethics_flag': 'No',\n",
       "     'ethics_review_area': [],\n",
       "     'soundness': '3 good',\n",
       "     'presentation': '3 good',\n",
       "     'contribution': '3 good',\n",
       "     'code_of_conduct': 'Yes'},\n",
       "    'signatures': ['NeurIPS.cc/2022/Conference/Paper2310/Reviewer_6d2t'],\n",
       "    'readers': ['everyone'],\n",
       "    'nonreaders': [],\n",
       "    'writers': ['NeurIPS.cc/2022/Conference',\n",
       "     'NeurIPS.cc/2022/Conference/Paper2310/Reviewer_6d2t']},\n",
       "   {'id': 'HNAOCsCSzZw',\n",
       "    'original': None,\n",
       "    'number': 3,\n",
       "    'cdate': 1657585011273,\n",
       "    'mdate': None,\n",
       "    'ddate': None,\n",
       "    'tcdate': 1657585011273,\n",
       "    'tmdate': 1657585011273,\n",
       "    'tddate': None,\n",
       "    'forum': 'zzDrPqn57DL',\n",
       "    'replyto': 'zzDrPqn57DL',\n",
       "    'invitation': 'NeurIPS.cc/2022/Conference/Paper2310/-/Official_Review',\n",
       "    'content': {'rating': '5: Borderline accept: Technically solid paper where reasons to accept outweigh reasons to reject, e.g., limited evaluation. Please use sparingly.',\n",
       "     'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.',\n",
       "     'summary': 'The paper proposes a framework for 3D detection from RGB and LiDAR inputs in autonomous driving scenes. The pipeline includes separate networks reasoning from RGB and LiDAR inputs independently, and uses a fusion network for refined detection when both sources are available. Also the paper considers situations of data corruption and proposed to boost the robustness in the model design. The paper is the first to identify and evaluate the problem that most existing methods do not consider situations where one or both sources are unavailable, and proposes a pipeline customized for this situation. The proposed method is evaluated in the standard settings of object detection and compared with baseline methods both qualitatively and quantitatively.',\n",
       "     'strengths_and_weaknesses': 'Strength:\\n\\n[1] The task identification. As mentioned above, the paper is the first to identify the issue within the current literature and models, and proposes a pipeline accordingly which reasons from two sources independently and thus more robust when data unavailability occurs. In this sense, the task identification itself is valuable to the community in defining and bringing attention to the task.\\n\\n[2] Extensive design choices and evaluation. Although the proposed pipeline is mostly based on existing methods, the paper is able to evaluate various design choices to demonstrate the flexibility of the proposed framework, as well as provide extensive evaluation into the results, yields SOTA results with both sources, and robust result when only one is available.\\n\\nWeakness:\\n\\n[1] Novelty and model design. The paper is novel in identifying the problem, which is legit and valuable. However for the proposed method itself, it is mostly a combination of existing methods utilization single sources without much modification, thus diminishing the merit of the proposed framework. Also the design to handle one or two sources in the framework is naive, basically running the first stage network only if only one source is available, and running both stages when two are available. A more sophisticated design could be, when for example camera stream is dropped for a few frames, is there a chance to stick to the fusion detector, but utilizing temporal information to compensate the missing RGB data, instead of simply drop the RGB branch and the fusion, running the LiDAR branch alone, which will likely result in a sudden drastic change to the detections?\\n\\n[2] Simulation for data corruption. The paper proposes to augment the data to simulate possible data corruption scenarios, via dropping points and limiting FOV. However more effort can be done to boost the robustness: e.g. looking for real driving sequences in extreme weather or with bad data, and train/evaluate on those data.',\n",
       "     'questions': 'Please see the points in the Weakness section above.',\n",
       "     'limitations': 'N/A',\n",
       "     'ethics_flag': 'No',\n",
       "     'ethics_review_area': [],\n",
       "     'soundness': '3 good',\n",
       "     'presentation': '3 good',\n",
       "     'contribution': '2 fair',\n",
       "     'code_of_conduct': 'Yes'},\n",
       "    'signatures': ['NeurIPS.cc/2022/Conference/Paper2310/Reviewer_2Tdc'],\n",
       "    'readers': ['everyone'],\n",
       "    'nonreaders': [],\n",
       "    'writers': ['NeurIPS.cc/2022/Conference',\n",
       "     'NeurIPS.cc/2022/Conference/Paper2310/Reviewer_2Tdc']},\n",
       "   {'id': 'uVn6Us0aP_',\n",
       "    'original': None,\n",
       "    'number': 4,\n",
       "    'cdate': 1657587291965,\n",
       "    'mdate': None,\n",
       "    'ddate': None,\n",
       "    'tcdate': 1657587291965,\n",
       "    'tmdate': 1657587291965,\n",
       "    'tddate': None,\n",
       "    'forum': 'zzDrPqn57DL',\n",
       "    'replyto': 'zzDrPqn57DL',\n",
       "    'invitation': 'NeurIPS.cc/2022/Conference/Paper2310/-/Official_Review',\n",
       "    'content': {'rating': '4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.',\n",
       "     'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.',\n",
       "     'summary': 'This paper introduced a method for point cloud object detection based on LiDAR camera fusion.  The main contribution of this method is the fusion framework that combines the camera and Lidar stream. This fusion module is very simple because it mainly consists of the concatenation of LiDAR and camera streams and a typical feature selection with an average pooling and 1x1 convolution. The results show that this method slightly outperformed the other method for the comparison. Moreover, the robustness against camera or LiDAR malfunctions is shown in the results. The ablation study shows that each module employed in this method improves performance. \\n',\n",
       "     'strengths_and_weaknesses': 'Strength\\n- The performance is slightly improved. \\n- The methodology is very simple. \\n\\nWeakness\\n- Considering the small performance improvement and the simple methodology, I would think that the contribution of this method is relatively limited. ',\n",
       "     'questions': 'I am wondering why this simple fusion method is better than the other methods compare in this paper. If there are some results and discussions that make sense would be helpful for the readers. ',\n",
       "     'limitations': 'I would suggest showing some failure cases and discussions about them because it will contribute to the community.\\n',\n",
       "     'ethics_flag': 'No',\n",
       "     'ethics_review_area': ['I don’t know'],\n",
       "     'soundness': '2 fair',\n",
       "     'presentation': '2 fair',\n",
       "     'contribution': '2 fair',\n",
       "     'code_of_conduct': 'Yes'},\n",
       "    'signatures': ['NeurIPS.cc/2022/Conference/Paper2310/Reviewer_fBas'],\n",
       "    'readers': ['everyone'],\n",
       "    'nonreaders': [],\n",
       "    'writers': ['NeurIPS.cc/2022/Conference',\n",
       "     'NeurIPS.cc/2022/Conference/Paper2310/Reviewer_fBas']},\n",
       "   {'id': 'Hd6Ce57Ircd',\n",
       "    'original': None,\n",
       "    'number': 5,\n",
       "    'cdate': 1658042263683,\n",
       "    'mdate': None,\n",
       "    'ddate': None,\n",
       "    'tcdate': 1658042263683,\n",
       "    'tmdate': 1658049110257,\n",
       "    'tddate': None,\n",
       "    'forum': 'zzDrPqn57DL',\n",
       "    'replyto': 'zzDrPqn57DL',\n",
       "    'invitation': 'NeurIPS.cc/2022/Conference/Paper2310/-/Official_Review',\n",
       "    'content': {'rating': '6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.',\n",
       "     'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.',\n",
       "     'summary': 'The authors propose a method to fuse two source of information for BEV detection namely multi-view images and LIDAR data in such a manner that any data defects in one source of information do not effect the network of the other method. Previous methods have combined the information from the two sources at different stages of the network pipeline, but they are prone to getting effected in the inference results when the data is corrupted from either source. This paper delays the combination of information to even later part of the pipeline thereby\\nmitigating the effect of bad/corrupted/unavailable data. They do so by generating a pseudo BEV point cloud just from multi-view cameras and combining that information with LIDAR BEV. The combination part is based on a dynamic fusion method which selects important fused features be it from camera based BEV or LIDAR based BEV. The results are shown where missing data in the LIDAR or camera image do not effect the detection unless both are missing for the same object in the scene e.g. car.',\n",
       "     'strengths_and_weaknesses': 'Strength:\\nThe paper addresses a problem which could be a real life problem in LIDAR or image based data capture, where the LIDAR data is missing due to 3D scene material/reflectance properties or the image could be missing from a video stream. These problems can cause existing networks to fail. This paper addresses this problem which can make the commercial deployment of such systems more doable. The paper is well written with clear explanation of the previous work. The results are detailed and show scenarios where they are better than previous best results in challenging situations.\\n\\nWeakness:\\n1. The dynamic fusion module should be explained in more detail as its one of the contributions of this paper. It should be explained with a scenario where data is missing from either of the streams and how the formulation in Eq.1 and Eq.2 will still be able to select the BEV information which exists to feed the final detection result.\\n2. Citations 44-45, 32-33, 57-58 are repeated citations. Please fix it.\\n4. grammar: #92 start->started, #3 discover->discovered',\n",
       "     'questions': 'For training the camera stream, a camera based BEV 2D point cloud is created. How is this camera stream trained as this may require a ground truth with camera based BEV 2D points and detections on that?\\n',\n",
       "     'limitations': \"The reviewer didn't find any major limitations. The authors could discuss and show some failure cases of their work.\",\n",
       "     'ethics_flag': 'No',\n",
       "     'ethics_review_area': [],\n",
       "     'soundness': '3 good',\n",
       "     'presentation': '3 good',\n",
       "     'contribution': '3 good',\n",
       "     'code_of_conduct': 'Yes'},\n",
       "    'signatures': ['NeurIPS.cc/2022/Conference/Paper2310/Reviewer_w9Mt'],\n",
       "    'readers': ['everyone'],\n",
       "    'nonreaders': [],\n",
       "    'writers': ['NeurIPS.cc/2022/Conference',\n",
       "     'NeurIPS.cc/2022/Conference/Paper2310/Reviewer_w9Mt']},\n",
       "   {'id': 'pqUNCBau8I',\n",
       "    'original': None,\n",
       "    'number': 7,\n",
       "    'cdate': 1659443987292,\n",
       "    'mdate': None,\n",
       "    'ddate': None,\n",
       "    'tcdate': 1659443987292,\n",
       "    'tmdate': 1659443987292,\n",
       "    'tddate': None,\n",
       "    'forum': 'zzDrPqn57DL',\n",
       "    'replyto': 'zzDrPqn57DL',\n",
       "    'invitation': 'NeurIPS.cc/2022/Conference/Paper2310/-/Official_Comment',\n",
       "    'content': {'title': '[Summary of Response] Thanks all reviewers for their thorough and insightful feedback!',\n",
       "     'comment': \"We thank all the reviewers for their time, insightful suggestions, and valuable comments. We are glad that all reviewers find our work is well motivated with valuable task definition (w9Mt, 2Tdc, 6d2t, CaaL),  simple and effective (w9Mt, fBas, 2Tdc, 6d2t, CaaL),with  clear explanation of the previous work (w9Mt, 2Tdc, CaaL), with comprehensive experiments on robustness (w9Mt, 2Tdc, 6d2t, CaaL), and detailed model explanation (6d2t, CaaL). \\n\\nBefore we respond to each reviewer's comments in detail, we revise the manuscript according to their suggestions, and we believe this makes our paper much stronger. Here is a list of changes we made:\\n\\n1. In Abstract and Sec.2, we fix the typo and related work discussion.\\n2. In Appendix C.1, we add an ablation study of Dynamic Fusion Module under robustness settings.\\n3. In Appendix C.2 and C.3, we add experiments for robustness analysis on both modality malfunctions and inferior image conditions.\\n4. In Appendix D, we add experiments on the performance gain based on the object distance range.\\n5. In Appendix E, we provide latency and memory footprint comparisons.\\n6. In Appendix F, we provide more visualization results for failure cases and analysis.\\n\\nNote that, in the revised version, we mark the text modification in blue.\"},\n",
       "    'signatures': ['NeurIPS.cc/2022/Conference/Paper2310/Authors'],\n",
       "    'readers': ['everyone'],\n",
       "    'nonreaders': [],\n",
       "    'writers': ['NeurIPS.cc/2022/Conference',\n",
       "     'NeurIPS.cc/2022/Conference/Paper2310/Authors']},\n",
       "   {'id': 'RWoipwU7F0',\n",
       "    'original': None,\n",
       "    'number': 1,\n",
       "    'cdate': 1661264951443,\n",
       "    'mdate': None,\n",
       "    'ddate': None,\n",
       "    'tcdate': 1661264951443,\n",
       "    'tmdate': 1661264951443,\n",
       "    'tddate': None,\n",
       "    'forum': 'zzDrPqn57DL',\n",
       "    'replyto': 'zzDrPqn57DL',\n",
       "    'invitation': 'NeurIPS.cc/2022/Conference/Paper2310/-/Meta_Review',\n",
       "    'content': {'metareview': 'The paper proposes a method to fuse two sources of information for Bird’s Eye View (BEV) detection, namely multi-view images and LIDAR data, in a way that any data defects in one source of information does not affect the other. Most existing camera-lidar fusion works decorate lidar points with image features and then perform detection in 3D/BEV space. This work leverages recent Lift-Splat-Shoot work for cameras, which allows one to map both camera and lidar inputs to BEV space, before fusing and applying the detection head. The reviewers appreciate the identification of the problem of present fusion methods that are susceptible to damage in one of the two sources of information, the simplicity of the method and its good empirical performance. They raise concerns regarding its novelty, given  the obvious choices of the present method. The rebuttal submitted by the authors presents more empirical results and ablations. Most reviewers appreciate the contribution of the paper, and  the paper is suggested for publication. ',\n",
       "     'recommendation': 'Accept',\n",
       "     'confidence': 'Certain',\n",
       "     'award': 'No'},\n",
       "    'signatures': ['NeurIPS.cc/2022/Conference/Paper2310/Area_Chair_2gWi'],\n",
       "    'readers': ['everyone'],\n",
       "    'nonreaders': [],\n",
       "    'writers': ['NeurIPS.cc/2022/Conference/Program_Chairs',\n",
       "     'NeurIPS.cc/2022/Conference/Paper2310/Area_Chairs'],\n",
       "    'pdate': None},\n",
       "   {'id': 'n0WgnRZKzkV',\n",
       "    'original': None,\n",
       "    'number': 1,\n",
       "    'cdate': 1663188480371,\n",
       "    'mdate': None,\n",
       "    'ddate': None,\n",
       "    'tcdate': 1663188480371,\n",
       "    'tmdate': 1663188480371,\n",
       "    'tddate': None,\n",
       "    'forum': 'zzDrPqn57DL',\n",
       "    'replyto': 'zzDrPqn57DL',\n",
       "    'invitation': 'NeurIPS.cc/2022/Conference/Paper2310/-/Decision',\n",
       "    'content': {'title': 'Paper Decision',\n",
       "     'decision': 'Accept',\n",
       "     'comment': ''},\n",
       "    'signatures': ['NeurIPS.cc/2022/Conference/Program_Chairs'],\n",
       "    'readers': ['everyone'],\n",
       "    'nonreaders': [],\n",
       "    'writers': ['NeurIPS.cc/2022/Conference/Program_Chairs']}]}}"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'papers_reviews_with_decision.csv' has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "# for NeurIPS 2022, the decision is stored in the /Decision invitation\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Function to convert timestamp to readable date\n",
    "def convert_timestamp(ts):\n",
    "    if ts is None:\n",
    "        return None\n",
    "    try:\n",
    "        return datetime.fromtimestamp(ts / 1000).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    except (ValueError, OSError):\n",
    "        return None\n",
    "\n",
    "# List to hold all paper records\n",
    "records = []\n",
    "\n",
    "for paper in papers:\n",
    "    record = {}\n",
    "    content = paper.content\n",
    "    \n",
    "    # Extract paper-level details\n",
    "    record['title'] = content.get('title', None)\n",
    "    record['keywords'] = \"; \".join(content.get('keywords', []))\n",
    "    record['TL;DR'] = content.get('TL;DR', None)\n",
    "    record['abstract'] = content.get('abstract', None)\n",
    "    record['cdate'] = convert_timestamp(paper.cdate)\n",
    "    record['tmdate'] = convert_timestamp(paper.tmdate)\n",
    "    \n",
    "    # Initialize reviewer fields\n",
    "    for i in range(1, 5):\n",
    "        prefix = f'reviewer{i}_'\n",
    "        record[f'{prefix}rating_score'] = None\n",
    "        record[f'{prefix}confidence_score'] = None\n",
    "        record[f'{prefix}summary'] = None\n",
    "        record[f'{prefix}strengths_and_weaknesses'] = None\n",
    "        record[f'{prefix}questions'] = None\n",
    "        record[f'{prefix}limitations'] = None\n",
    "        record[f'{prefix}soundness'] = None\n",
    "        record[f'{prefix}presentation'] = None\n",
    "        record[f'{prefix}contribution'] = None\n",
    "        record[f'{prefix}length'] = None\n",
    "        record[f'{prefix}cdate'] = None\n",
    "        record[f'{prefix}tmdate'] = None\n",
    "    \n",
    "    # Initialize decision field\n",
    "    record['decision'] = None\n",
    "    \n",
    "    # Extract reviews and decision\n",
    "    reviews = paper.details.get('directReplies', [])\n",
    "    for reply in reviews:\n",
    "        invitation = reply.get('invitation', '')\n",
    "        content_reply = reply.get('content', {})\n",
    "        \n",
    "        if '/Official_Review' in invitation:\n",
    "            # It's a review\n",
    "            # Find the index based on existing filled reviewers\n",
    "            reviewer_index = None\n",
    "            for i in range(1, 5):\n",
    "                prefix = f'reviewer{i}_'\n",
    "                if record[f'{prefix}rating_score'] is None:\n",
    "                    reviewer_index = i\n",
    "                    break\n",
    "            if reviewer_index is not None:\n",
    "                prefix = f'reviewer{reviewer_index}_'\n",
    "                record[f'{prefix}rating_score'] = content_reply.get('rating', None)\n",
    "                record[f'{prefix}confidence_score'] = content_reply.get('confidence', None)\n",
    "                record[f'{prefix}summary'] = content_reply.get('summary', None)\n",
    "                record[f'{prefix}strengths_and_weaknesses'] = content_reply.get('strengths_and_weaknesses', None)\n",
    "                record[f'{prefix}questions'] = content_reply.get('questions', None)\n",
    "                record[f'{prefix}limitations'] = content_reply.get('limitations', None)\n",
    "                record[f'{prefix}soundness'] = content_reply.get('soundness', None)\n",
    "                record[f'{prefix}presentation'] = content_reply.get('presentation', None)\n",
    "                record[f'{prefix}contribution'] = content_reply.get('contribution', None)\n",
    "                \n",
    "                # Calculate the length of the review\n",
    "                review_text = ' '.join([\n",
    "                    content_reply.get('summary', ''),\n",
    "                    content_reply.get('strengths_and_weaknesses', ''),\n",
    "                    content_reply.get('questions', ''),\n",
    "                    content_reply.get('limitations', '')\n",
    "                ])\n",
    "                record[f'{prefix}length'] = len(review_text)\n",
    "                \n",
    "                # Extract review dates\n",
    "                record[f'{prefix}cdate'] = convert_timestamp(reply.get('cdate', None))\n",
    "                record[f'{prefix}tmdate'] = convert_timestamp(reply.get('tmdate', None))\n",
    "        \n",
    "        elif '/Decision' in invitation:\n",
    "            # It's the decision\n",
    "            record['decision'] = content_reply.get('decision', None)\n",
    "    \n",
    "    records.append(record)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Optional: Reorder columns for better readability\n",
    "# Start with paper details\n",
    "paper_columns = ['title', 'keywords', 'TL;DR', 'abstract', 'cdate', 'tmdate']\n",
    "# Then reviewer details\n",
    "reviewer_columns = []\n",
    "for i in range(1, 5):\n",
    "    prefix = f'reviewer{i}_'\n",
    "    reviewer_columns.extend([\n",
    "        f'{prefix}rating_score',\n",
    "        f'{prefix}confidence_score',\n",
    "        f'{prefix}summary',\n",
    "        f'{prefix}strengths_and_weaknesses',\n",
    "        f'{prefix}questions',\n",
    "        f'{prefix}limitations',\n",
    "        f'{prefix}soundness',\n",
    "        f'{prefix}presentation',\n",
    "        f'{prefix}contribution',\n",
    "        f'{prefix}length',\n",
    "        f'{prefix}cdate',\n",
    "        f'{prefix}tmdate'\n",
    "    ])\n",
    "# Add decision as the last column\n",
    "all_columns = paper_columns + reviewer_columns + ['decision']\n",
    "df = df[all_columns]\n",
    "\n",
    "# Export to CSV\n",
    "df.to_csv(\"./data/\" + conference_id.replace('/',\"-\") + '.csv', index=False)\n",
    "\n",
    "print(\"CSV file 'papers_reviews_with_decision.csv' has been created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Getting V1 Notes: 100%|█████████▉| 3792/3796 [00:06<00:00, 617.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'papers_reviews_with_decision.csv' has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "import openreview\n",
    "# client = openreview.api.OpenReviewClient(baseurl='https://api2.openreview.net')\n",
    "client = openreview.Client(baseurl='https://api.openreview.net')\n",
    "\n",
    "conference_id = 'ICLR.cc/2023/Conference'\n",
    "papers = client.get_all_notes(\n",
    "    invitation=f'{conference_id}/-/Blind_Submission',\n",
    "    details='directReplies', \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'zzqBoIFOQ1',\n",
       " 'original': 'dC_9j4aLwcA',\n",
       " 'number': 3283,\n",
       " 'cdate': 1663850184200,\n",
       " 'pdate': 1675279800000,\n",
       " 'odate': 1664468100000,\n",
       " 'mdate': None,\n",
       " 'tcdate': 1663850184200,\n",
       " 'tmdate': 1732526020341,\n",
       " 'ddate': None,\n",
       " 'content': {'title': 'Guiding Safe Exploration with Weakest Preconditions',\n",
       "  'authorids': ['~Greg_Anderson1', '~Swarat_Chaudhuri1', '~Isil_Dillig1'],\n",
       "  'authors': ['Greg Anderson', 'Swarat Chaudhuri', 'Isil Dillig'],\n",
       "  'keywords': ['reinforcement learning', 'safe learning', 'safe exploration'],\n",
       "  'TL;DR': 'We use an online, weakest-precondition-based approach to ensure safety during exploration without interfering with performance.',\n",
       "  'abstract': 'In reinforcement learning for safety-critical settings, it is often desirable for the agent to obey safety constraints at all points in time, including during training. We present a novel neurosymbolic approach called SPICE to solve this safe exploration problem. SPICE uses an online shielding layer based on symbolic weakest preconditions to achieve a more precise safety analysis than existing tools without unduly impacting the training process. We evaluate the approach on a suite of continuous control benchmarks and show that it can achieve comparable performance to existing safe learning techniques while incurring fewer safety violations. Additionally, we present theoretical results showing that SPICE converges to the optimal safe policy under reasonable assumptions.',\n",
       "  'anonymous_url': 'I certify that there is no URL (e.g., github page) that could be used to find authors’ identity.',\n",
       "  'no_acknowledgement_section': 'I certify that there is no acknowledgement section in this submission for double blind review.',\n",
       "  'code_of_ethics': 'I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics',\n",
       "  'submission_guidelines': 'Yes',\n",
       "  'resubmission': '',\n",
       "  'student_author': '',\n",
       "  'Please_choose_the_closest_area_that_your_submission_falls_into': 'Reinforcement Learning (eg, decision and control, planning, hierarchical RL, robotics)',\n",
       "  'paperhash': 'anderson|guiding_safe_exploration_with_weakest_preconditions',\n",
       "  'pdf': '/pdf/31fba2ce53b7314f1c3b3ec7719c818d82414a6d.pdf',\n",
       "  'supplementary_material': '/attachment/ec3f34f04957b594c28e243d8aa5403d15fd76b5.zip',\n",
       "  '_bibtex': '@inproceedings{\\nanderson2023guiding,\\ntitle={Guiding Safe Exploration with Weakest Preconditions},\\nauthor={Greg Anderson and Swarat Chaudhuri and Isil Dillig},\\nbooktitle={The Eleventh International Conference on Learning Representations },\\nyear={2023},\\nurl={https://openreview.net/forum?id=zzqBoIFOQ1}\\n}',\n",
       "  'venue': 'ICLR 2023 poster',\n",
       "  'venueid': 'ICLR.cc/2023/Conference',\n",
       "  'community_implementations': '[![CatalyzeX](/images/catalyzex_icon.svg) 1 code implementation](https://www.catalyzex.com/paper/guiding-safe-exploration-with-weakest/code)'},\n",
       " 'forum': 'zzqBoIFOQ1',\n",
       " 'referent': None,\n",
       " 'invitation': 'ICLR.cc/2023/Conference/-/Blind_Submission',\n",
       " 'replyto': None,\n",
       " 'readers': ['everyone'],\n",
       " 'nonreaders': [],\n",
       " 'signatures': ['ICLR.cc/2023/Conference'],\n",
       " 'writers': ['ICLR.cc/2023/Conference'],\n",
       " 'details': {'directReplies': [{'id': 'MXV1TACjo_',\n",
       "    'original': None,\n",
       "    'number': 1,\n",
       "    'cdate': 1666251648361,\n",
       "    'mdate': None,\n",
       "    'ddate': None,\n",
       "    'tcdate': 1666251648361,\n",
       "    'tmdate': 1669196188530,\n",
       "    'tddate': None,\n",
       "    'forum': 'zzqBoIFOQ1',\n",
       "    'replyto': 'zzqBoIFOQ1',\n",
       "    'invitation': 'ICLR.cc/2023/Conference/Paper3283/-/Official_Review',\n",
       "    'content': {'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.',\n",
       "     'summary_of_the_paper': 'This paper deals with safe exploration in reinforcement learning in which an agent is required to ensure safety during training. The authors present a neuro-symbolic approach called SPICE. based on symbolic weakest preconditions. Empirically, they evaluate their approach on toy benchmarks, and show that it is able to achieve comparable performance to existing safe learning techniques while incurring fewer safety violations. ',\n",
       "     'strength_and_weaknesses': \"### Strength\\n- Interesting and importance problem formulation on safe exploration.\\n- Unique approach for safe exploration. As far as I know, there is no existing method using weakest precondition.\\n\\n### Weakness\\n- Though the number and quality of baselines looks sufficient to me, benchmark problem is very easy. I think the authors should have tested their method in MuJoCo or SafetyGym benchmarks. The proposed method depends on environment approximation; hence, it is particularly important to know whether it is applicable to non-linear complicated system (I think even negative results are ok, and it is significant to know the limitations).\\n- Low applicability to real problems. As the authors discuss the experimental results in the first paragraph on page 8, the proposed method works better than baselines when the environment can be approximated with linear equations. I think this is a huge limitation of the proposed method. I personally recommend the authors to connect this work with Linear MDP (https://arxiv.org/pdf/1907.05388.pdf), which would provide much richer theoretical results and good empirical results under the assumption of feature mapping functions.\\n\\n### Questions\\n- How did the author choose weakest preconditions for each experiments? Is it possible to decrease the constraints violation while tuning the parameter of the weakest preconditions?\\n\\n### Other comments\\n- The equation in Section 2 should be rewritten\\n    - (Section 2) $\\\\text{Safe}(\\\\pi)$ is not defined.\\n    - Is $P_{x∼S{_{\\\\pi_i}}}(x \\\\in S_U ) < \\\\delta$ really what the authors want to represent? I think the probability of an agent's trajectory being safe is $(1-\\\\delta)^N$ when the length of episode is $N$. I guess the authors should have represented in other equations.\",\n",
       "     'clarity,_quality,_novelty_and_reproducibility': '### Clarity\\nThis paper is mostly well-written and easy to follow.\\n\\n### Quality\\nI have several concerns regarding the quality of experiments and method itself. Especially, I am not fully convinced whether this proposed method is really useful or not.\\n\\n### Novelty\\nAs far as I know, the proposed approach using weakest precondition is new.\\n\\n### Reproducibility\\nThe source-doe is attached, and reproducibility is high.',\n",
       "     'summary_of_the_review': 'Though this paper presents a new, promising method for an interesting problems, there are several issues and concerns about the applicability and empirical evaluations. Hence, I recommend rejection for now.',\n",
       "     'correctness': '2: Several of the paper’s claims are incorrect or not well-supported.',\n",
       "     'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.',\n",
       "     'empirical_novelty_and_significance': '2: The contributions are only marginally significant or novel.',\n",
       "     'flag_for_ethics_review': ['NO.'],\n",
       "     'recommendation': '6: marginally above the acceptance threshold'},\n",
       "    'signatures': ['ICLR.cc/2023/Conference/Paper3283/Reviewer_aN3N'],\n",
       "    'readers': ['everyone'],\n",
       "    'nonreaders': [],\n",
       "    'writers': ['ICLR.cc/2023/Conference',\n",
       "     'ICLR.cc/2023/Conference/Paper3283/Reviewer_aN3N']},\n",
       "   {'id': 'AQTBBXOc898',\n",
       "    'original': None,\n",
       "    'number': 2,\n",
       "    'cdate': 1666651351449,\n",
       "    'mdate': None,\n",
       "    'ddate': None,\n",
       "    'tcdate': 1666651351449,\n",
       "    'tmdate': 1666651351449,\n",
       "    'tddate': None,\n",
       "    'forum': 'zzqBoIFOQ1',\n",
       "    'replyto': 'zzqBoIFOQ1',\n",
       "    'invitation': 'ICLR.cc/2023/Conference/Paper3283/-/Official_Review',\n",
       "    'content': {'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.',\n",
       "     'summary_of_the_paper': 'This paper adapts ideas from formal program analysis, i.e. weakest preconditions, to the constrained  reinforcement learning. Here an emphasis is made for (i) working in an a-priori unknown MDP and (ii) minimizing constraint violations during deployment *and* training. \\n\\nThe key idea is to alternate (i) shielded exploration (ii) system identification/ model learning and (iii) computing a new avoid predicate for the shield. Again, this is done by adapting a classic idea from program analysis (called weakest preconditions) where one tries to identify the largest set such of inputs (the precondition) that guarantees the property (here not violating the constraint) is satisfied.\\n\\nTechnically, this by linearizing the learned model and learning a (union) of precondition polyhedra. These polyhedra are then used to project the actions proposed by the learned control policy to a safe action (according the learned model).\\n\\nEmpirically this seems to be very effective and future extensions are proposed (using non-linear models) that would likely yield unambigously state-of-the-art performance.',\n",
       "     'strength_and_weaknesses': '# Strengths\\n\\nThe technique is well motivated and a good proof of concept regarding combining model based safety analysis with model free RL. There is a clear path towards having robust theoretical guarantees (see weaknesses below) and a there is a literature of techniques on abstract interpretation that are likely to improve the representations of the preconditions.\\n\\n# Weaknesses\\n\\n1. Some of the assumptions in the theorems seems difficult to realize. In particular, because the learned safety shield biases the exploration, it seems difficult to assert that the final model will always achieve a given PAC bound. Perhaps I am missing something.\\n\\n2. The analysis is done using a linearization of the model. This limits the class of models for which the shield can accurately forecast safety. This is acknowledged in the paper and the proposed directions for mitigation seem reasonable.\\n\\n3. Missing from the related work seems to the literature on abstract interpretation that (from my outsider knowledge) offers a similar promise of learning pre-conditions for safety. Since the preconditions considered in this work are (under?) approximations for the learned model, it seems good to compare.',\n",
       "     'clarity,_quality,_novelty_and_reproducibility': 'The technique and motivation are clearly communicated and the algorithm is (to my knowledge) distinct from other shielding techniques. While the individual building blocks are well established, this seems to be a novel application well worth studying.',\n",
       "     'summary_of_the_review': 'The paper contributes a novel algorithm for creating safety shields during the training of RL-agents. It provides a nice demonstration of the power of alternating model learning and state-of-the-art RL techniques for safe RL. In particular, the paper highlights how model learning in this setting need only deal with qualitative semantics (i.e., is it possible to reach state s) and can let the RL algorithm handle the quantitative optimizations.',\n",
       "     'correctness': '4: All of the claims and statements are well-supported and correct.',\n",
       "     'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.',\n",
       "     'empirical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.',\n",
       "     'flag_for_ethics_review': ['NO.'],\n",
       "     'recommendation': '8: accept, good paper'},\n",
       "    'signatures': ['ICLR.cc/2023/Conference/Paper3283/Reviewer_PYp3'],\n",
       "    'readers': ['everyone'],\n",
       "    'nonreaders': [],\n",
       "    'writers': ['ICLR.cc/2023/Conference',\n",
       "     'ICLR.cc/2023/Conference/Paper3283/Reviewer_PYp3']},\n",
       "   {'id': 'aib0K4Qbbj',\n",
       "    'original': None,\n",
       "    'number': 3,\n",
       "    'cdate': 1666735486524,\n",
       "    'mdate': None,\n",
       "    'ddate': None,\n",
       "    'tcdate': 1666735486524,\n",
       "    'tmdate': 1666735486524,\n",
       "    'tddate': None,\n",
       "    'forum': 'zzqBoIFOQ1',\n",
       "    'replyto': 'zzqBoIFOQ1',\n",
       "    'invitation': 'ICLR.cc/2023/Conference/Paper3283/-/Official_Review',\n",
       "    'content': {'confidence': '3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.',\n",
       "     'summary_of_the_paper': 'The paper proposes a method for safe reinforcement learning whereby it uses a\\nlearnt environment to not only optimise policies but also to improve safe\\nexploration. Concretely, this is realised by taking a linear approximation of\\nthe environment, which in conjunction with the safety specification gives\\nlinear constraints (called weakest preconditions in the paper) on the actions\\nthat are safe to be performed.',\n",
       "     'strength_and_weaknesses': '+ The approach reduces the number of safety violations during training when\\ncompared with the state-of-the-art in the area.\\n\\n- Somewhat straightforward and incremental: straightforward in that it relies\\n  on linear approximations of the environment and incremental in that the\\n  contribution is limited to the derivation of safety predicates within a\\n  previously studied framework.',\n",
       "     'clarity,_quality,_novelty_and_reproducibility': 'The paper is overall well written and presented. The contribution includes some\\nnovel aspects pertaining to the computation of the weakest preconditions. The\\nsignificance of the overall method is adequately evaluated.',\n",
       "     'summary_of_the_review': 'The paper builds on previous work on safe reinforcement learning  to derive a\\nmethod which although makes strong, linearity assumptions on the environment is\\nable to outperform the state-of-the-art in terms of the number of safety\\nviolations during training.',\n",
       "     'correctness': '4: All of the claims and statements are well-supported and correct.',\n",
       "     'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.',\n",
       "     'empirical_novelty_and_significance': '2: The contributions are only marginally significant or novel.',\n",
       "     'flag_for_ethics_review': ['NO.'],\n",
       "     'recommendation': '6: marginally above the acceptance threshold'},\n",
       "    'signatures': ['ICLR.cc/2023/Conference/Paper3283/Reviewer_HV3z'],\n",
       "    'readers': ['everyone'],\n",
       "    'nonreaders': [],\n",
       "    'writers': ['ICLR.cc/2023/Conference',\n",
       "     'ICLR.cc/2023/Conference/Paper3283/Reviewer_HV3z']},\n",
       "   {'id': 'd3-kXa-Agu',\n",
       "    'original': None,\n",
       "    'number': 4,\n",
       "    'cdate': 1667459319188,\n",
       "    'mdate': None,\n",
       "    'ddate': None,\n",
       "    'tcdate': 1667459319188,\n",
       "    'tmdate': 1670985996556,\n",
       "    'tddate': None,\n",
       "    'forum': 'zzqBoIFOQ1',\n",
       "    'replyto': 'zzqBoIFOQ1',\n",
       "    'invitation': 'ICLR.cc/2023/Conference/Paper3283/-/Official_Review',\n",
       "    'content': {'confidence': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.',\n",
       "     'summary_of_the_paper': 'The paper proposes a safe RL algorithm, Symbolic Preconditions for Constrained Exploration (SPICE), that uses a model-based shielding mechanism to produce safe actions. A linearization of the model and polyhedral state constraints are used to determine which actions are safe, and then the proposed action is projected onto the space of safe actions. The paper provides a regret bound for the algorithm and demonstrates that SPICE can attain fewer safety violations during training than model-free baselines.',\n",
       "     'strength_and_weaknesses': 'Strengths:\\n* SPICE’s use of formal methods provides strong safety guarantees if the assumptions hold. This is very desirable in safety critical applications.\\n* In experiments, SPICE substantially reduces the number of violations substantially compared to CPO and CSC.\\n\\nWeaknesses:\\n* A footnote states \"We use a modified version of our approach instead of comparing to Bharadhwaj et al. (2021) directly because the code for that paper is unavailable.\" While this is reasonable, the present paper does not provide a description of your modifications. It is difficult for the reader to compare the performance of SPICE vs. “CSC” without knowing exactly what “CSC” means.\\n* From Figure 3, the variance of the policy’s performance appears extremely high for both SPICE and CSC. The plots in the CSC paper look much less noisy. This raises questions about the quality of the implementation.\\n* SPICE converges to a suboptimal policy (worse than CPO) in many cases.\\n* The experiments do not compare to any other model-based safe RL algorithms.\\n* Linearization of the model may limit what types of problems SPICE can solve effectively.',\n",
       "     'clarity,_quality,_novelty_and_reproducibility': 'The paper is generally clear. I appreciate the use of examples to demonstrate the approach.\\nHowever, the statement of theorem 2 should be improved:\\n* Some of the symbols that appear in the regret bound are only defined in the appendix ($L_R$, $\\\\sigma$) or in the following text ($\\\\zeta$).\\n* To be precise, $\\\\epsilon_m$ and $\\\\epsilon_\\\\pi$ are upper bounds on the divergences that hold for all $T$. (This is stated in the appendix, but not in the main text.) The actual divergences are changing throughout training as the model and policy are updated.\\n\\nThe proposed approach is novel, to my knowledge.\\n\\nRegarding reproducibility, the lack of details regarding the CSC implementation is a significant concern.\\n\\nThe paper is missing some relevant references to model-based safe RL papers, such as\\n* Safe Reinforcement Learning Using Robust MPC. M. Zanon, S. Gros\\n* Safe Reinforcement Learning by Imagining the Near Future. G. Thomas, Y. Luo, T. Ma\\n',\n",
       "     'summary_of_the_review': 'I like the proposed approach and its associated guarantees. My main issues are in the experiments, as detailed above. The state of the experiments makes it hard to recommend acceptance in the paper’s current form, in spite of other positive attributes of the paper.',\n",
       "     'correctness': '4: All of the claims and statements are well-supported and correct.',\n",
       "     'technical_novelty_and_significance': '3: The contributions are significant and somewhat new. Aspects of the contributions exist in prior work.',\n",
       "     'empirical_novelty_and_significance': '2: The contributions are only marginally significant or novel.',\n",
       "     'flag_for_ethics_review': ['NO.'],\n",
       "     'recommendation': '6: marginally above the acceptance threshold'},\n",
       "    'signatures': ['ICLR.cc/2023/Conference/Paper3283/Reviewer_euEh'],\n",
       "    'readers': ['everyone'],\n",
       "    'nonreaders': [],\n",
       "    'writers': ['ICLR.cc/2023/Conference',\n",
       "     'ICLR.cc/2023/Conference/Paper3283/Reviewer_euEh']},\n",
       "   {'id': 'uj_ZPbmWO-',\n",
       "    'original': None,\n",
       "    'number': 1,\n",
       "    'cdate': 1674241783547,\n",
       "    'pdate': None,\n",
       "    'mdate': None,\n",
       "    'ddate': None,\n",
       "    'tcdate': 1674241783547,\n",
       "    'tmdate': 1675383061888,\n",
       "    'tddate': None,\n",
       "    'forum': 'zzqBoIFOQ1',\n",
       "    'replyto': 'zzqBoIFOQ1',\n",
       "    'invitation': 'ICLR.cc/2023/Conference/Paper3283/-/Decision',\n",
       "    'content': {'title': 'Paper Decision',\n",
       "     'decision': 'Accept: poster',\n",
       "     'metareview:_summary,_strengths_and_weaknesses': 'This paper provides a novel and potentially important strategy for using the technique of weakest preconditions, from the CS verification literature, to constrain exploration during reinforcement learning, and provide safety guarantees.\\nBy making an analytical model of the preconditions of a function, it is possible to efficiently, analytically chain the conditions to compute an h-step precondition of a safety condition.  This is used as a \"shield\" during exploration.\\n\\nThe reviewers were generally positive about the ideas in the paper but had some substantial concerns.  The major concerns were:\\n- Weaknesses in experimental methodology:  \\n  o the apparent variance in the training curves for SPICE and CSC seems huge (both in main results and in appendix):  this requires some exploration and explanation\\n  o it might have been good, to compare your replication of CSC against their published results as a sanity check\\n  o it would have been good to try some harder RL problems\\n- It would have been good to make some of the assumptions of the theorems clearer in the main body of the paper.\\n  o the assumption of approximate correctness of the learned model is very strong\\n  o there is a general concern that the linear approximation may not model the system well enough\\n\\nThe discussion in the appendix about the effect of changing the horizon was interesting.  Would be nice to put some discussion of that in main paper if possible. After rebuttal, authors have addressed most of the concerns, so we recommend to accept.',\n",
       "     'summary_of_AC-reviewer_meeting': 'Reviewers with more expertise in the verification community found the methods to be relatively straightforward and incremental. Reviewers with more expertise in the RL community were concerned about experimental results (high variance, simple domains), but rebuttal has addressed many of them. There was agreement that the approach was interesting.',\n",
       "     'justification_for_why_not_higher_score': 'There were substantial concerns from all reviewers.',\n",
       "     'justification_for_why_not_lower_score': 'N/A'},\n",
       "    'signatures': ['ICLR.cc/2023/Conference/Program_Chairs'],\n",
       "    'readers': ['everyone'],\n",
       "    'nonreaders': [],\n",
       "    'writers': ['ICLR.cc/2023/Conference/Program_Chairs']}]}}"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers[0].__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file 'papers_reviews_with_decision.csv' has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "# for ICLR 2023, the decision is stored in the /Decision invitation\n",
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Function to convert timestamp to readable date\n",
    "def convert_timestamp(ts):\n",
    "    if ts is None:\n",
    "        return None\n",
    "    try:\n",
    "        return datetime.fromtimestamp(ts / 1000).strftime('%Y-%m-%d %H:%M:%S')\n",
    "    except (ValueError, OSError):\n",
    "        return None\n",
    "\n",
    "# List to hold all paper records\n",
    "records = []\n",
    "\n",
    "for paper in papers:\n",
    "    record = {}\n",
    "    content = paper.content\n",
    "    \n",
    "    # Extract paper-level details\n",
    "    record['title'] = content.get('title', None)\n",
    "    record['keywords'] = \"; \".join(content.get('keywords', []))\n",
    "    record['TL;DR'] = content.get('TL;DR', None)\n",
    "    record['abstract'] = content.get('abstract', None)\n",
    "    record['cdate'] = convert_timestamp(paper.cdate)\n",
    "    record['tmdate'] = convert_timestamp(paper.tmdate)\n",
    "    \n",
    "    # Initialize reviewer fields (up to 4 reviewers)\n",
    "    for i in range(1, 5):\n",
    "        prefix = f'reviewer{i}_'\n",
    "        record[f'{prefix}recommendation'] = None\n",
    "        record[f'{prefix}confidence'] = None\n",
    "        record[f'{prefix}summary_of_the_paper'] = None\n",
    "        record[f'{prefix}strength_and_weaknesses'] = None\n",
    "        record[f'{prefix}clarity_quality_novelty_and_reproducibility'] = None\n",
    "        record[f'{prefix}summary_of_the_review'] = None\n",
    "        record[f'{prefix}correctness'] = None\n",
    "        record[f'{prefix}technical_novelty_and_significance'] = None\n",
    "        record[f'{prefix}empirical_novelty_and_significance'] = None\n",
    "        record[f'{prefix}flag_for_ethics_review'] = None\n",
    "        record[f'{prefix}length'] = None\n",
    "        record[f'{prefix}cdate'] = None\n",
    "        record[f'{prefix}tmdate'] = None\n",
    "\n",
    "    # Initialize decision field\n",
    "    record['decision'] = None\n",
    "\n",
    "    # Extract reviews and decision\n",
    "    reviews = paper.details.get('directReplies', [])\n",
    "    for reply in reviews:\n",
    "        invitation = reply.get('invitation', '')\n",
    "        content_reply = reply.get('content', {})\n",
    "\n",
    "        if '/Official_Review' in invitation:\n",
    "            # It's a review\n",
    "            # Find the first available reviewer slot\n",
    "            reviewer_index = None\n",
    "            for i in range(1, 5):\n",
    "                prefix = f'reviewer{i}_'\n",
    "                # Check if 'recommendation' is None to assign a new reviewer\n",
    "                if record.get(f'{prefix}recommendation') is None:\n",
    "                    reviewer_index = i\n",
    "                    break\n",
    "            if reviewer_index is not None:\n",
    "                prefix = f'reviewer{reviewer_index}_'\n",
    "                # Assign fields directly as per ICLR field names\n",
    "                record[f'{prefix}recommendation'] = content_reply.get('recommendation', None)\n",
    "                record[f'{prefix}confidence'] = content_reply.get('confidence', None)\n",
    "                record[f'{prefix}summary_of_the_paper'] = content_reply.get('summary_of_the_paper', None)\n",
    "                record[f'{prefix}strength_and_weaknesses'] = content_reply.get('strength_and_weaknesses', None)\n",
    "                record[f'{prefix}clarity_quality_novelty_and_reproducibility'] = content_reply.get('clarity,_quality,_novelty_and_reproducibility', None)\n",
    "                record[f'{prefix}summary_of_the_review'] = content_reply.get('summary_of_the_review', None)\n",
    "                record[f'{prefix}correctness'] = content_reply.get('correctness', None)\n",
    "                record[f'{prefix}technical_novelty_and_significance'] = content_reply.get('technical_novelty_and_significance', None)\n",
    "                record[f'{prefix}empirical_novelty_and_significance'] = content_reply.get('empirical_novelty_and_significance', None)\n",
    "                record[f'{prefix}flag_for_ethics_review'] = \", \".join(content_reply.get('flag_for_ethics_review', [])) if isinstance(content_reply.get('flag_for_ethics_review', []), list) else content_reply.get('flag_for_ethics_review', None)\n",
    "\n",
    "                # Calculate the length of the review\n",
    "                review_text = ' '.join([\n",
    "                    content_reply.get('summary_of_the_paper', ''),\n",
    "                    content_reply.get('strength_and_weaknesses', ''),\n",
    "                    content_reply.get('clarity,_quality,_novelty_and_reproducibility', ''),\n",
    "                    content_reply.get('summary_of_the_review', ''),\n",
    "                    content_reply.get('correctness', ''),\n",
    "                    content_reply.get('technical_novelty_and_significance', ''),\n",
    "                    content_reply.get('empirical_novelty_and_significance', ''),\n",
    "                ])\n",
    "                record[f'{prefix}length'] = len(review_text)\n",
    "\n",
    "                # Extract review dates\n",
    "                record[f'{prefix}cdate'] = convert_timestamp(reply.get('cdate', None))\n",
    "                record[f'{prefix}tmdate'] = convert_timestamp(reply.get('tmdate', None))\n",
    "        \n",
    "        elif '/Decision' in invitation:\n",
    "            # It's the decision\n",
    "            record['decision'] = content_reply.get('decision', None)\n",
    "    \n",
    "    records.append(record)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Optional: Reorder columns for better readability\n",
    "# Start with paper details\n",
    "paper_columns = ['title', 'keywords', 'TL;DR', 'abstract', 'cdate', 'tmdate']\n",
    "# Then reviewer details\n",
    "reviewer_columns = []\n",
    "for i in range(1, 5):\n",
    "    prefix = f'reviewer{i}_'\n",
    "    reviewer_columns.extend([\n",
    "        f'{prefix}recommendation',\n",
    "        f'{prefix}confidence',\n",
    "        f'{prefix}summary_of_the_paper',\n",
    "        f'{prefix}strength_and_weaknesses',\n",
    "        f'{prefix}clarity_quality_novelty_and_reproducibility',\n",
    "        f'{prefix}summary_of_the_review',\n",
    "        f'{prefix}correctness',\n",
    "        f'{prefix}technical_novelty_and_significance',\n",
    "        f'{prefix}empirical_novelty_and_significance',\n",
    "        f'{prefix}flag_for_ethics_review',\n",
    "        f'{prefix}length',\n",
    "        f'{prefix}cdate',\n",
    "        f'{prefix}tmdate'\n",
    "    ])\n",
    "# Add decision as the last column\n",
    "all_columns = paper_columns + reviewer_columns + ['decision']\n",
    "df = df[all_columns]\n",
    "\n",
    "# Export to CSV\n",
    "df.to_csv(\"./data/\" + conference_id.replace('/',\"-\") + '.csv', index=False)\n",
    "\n",
    "print(\"CSV file 'papers_reviews_with_decision.csv' has been created successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
